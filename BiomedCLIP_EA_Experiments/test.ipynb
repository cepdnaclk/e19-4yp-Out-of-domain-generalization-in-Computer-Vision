{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37006710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/embedding-gecko-001', display_name='Embedding Gecko', description='Obtain a distributed representation of a text.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1024, output_token_limit=1, supported_actions=['embedText', 'countTextTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.0-pro-vision-latest', display_name='Gemini 1.0 Pro Vision', description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=12288, output_token_limit=4096, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-pro-vision', display_name='Gemini 1.0 Pro Vision', description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=12288, output_token_limit=4096, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-pro-latest', display_name='Gemini 1.5 Pro Latest', description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=2000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-pro-001', display_name='Gemini 1.5 Pro 001', description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=2000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-pro-002', display_name='Gemini 1.5 Pro 002', description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.', version='002', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=2000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-pro', display_name='Gemini 1.5 Pro', description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=2000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-latest', display_name='Gemini 1.5 Flash Latest', description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-001', display_name='Gemini 1.5 Flash 001', description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-001-tuning', display_name='Gemini 1.5 Flash 001 Tuning', description='Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=16384, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createTunedModel'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash', display_name='Gemini 1.5 Flash', description='Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-002', display_name='Gemini 1.5 Flash 002', description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.', version='002', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-8b', display_name='Gemini 1.5 Flash-8B', description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['createCachedContent', 'generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-8b-001', display_name='Gemini 1.5 Flash-8B 001', description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['createCachedContent', 'generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-8b-latest', display_name='Gemini 1.5 Flash-8B Latest', description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['createCachedContent', 'generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-8b-exp-0827', display_name='Gemini 1.5 Flash 8B Experimental 0827', description='Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-1.5-flash-8b-exp-0924', display_name='Gemini 1.5 Flash 8B Experimental 0924', description='Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1000000, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-pro-exp-03-25', display_name='Gemini 2.5 Pro Experimental 03-25', description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro', version='2.5-exp-03-25', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-pro-preview-03-25', display_name='Gemini 2.5 Pro Preview 03-25', description='Gemini 2.5 Pro Preview 03-25', version='2.5-preview-03-25', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-flash-preview-04-17', display_name='Gemini 2.5 Flash Preview 04-17', description='Preview release (April 17th, 2025) of Gemini 2.5 Flash', version='2.5-preview-04-17', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-flash-preview-05-20', display_name='Gemini 2.5 Flash Preview 05-20', description='Preview release (April 17th, 2025) of Gemini 2.5 Flash', version='2.5-preview-05-20', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-flash-preview-04-17-thinking', display_name='Gemini 2.5 Flash Preview 04-17 for cursor testing', description='Preview release (April 17th, 2025) of Gemini 2.5 Flash', version='2.5-preview-04-17', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-pro-preview-05-06', display_name='Gemini 2.5 Pro Preview 05-06', description='Preview release (May 6th, 2025) of Gemini 2.5 Pro', version='2.5-preview-05-06', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-exp', display_name='Gemini 2.0 Flash Experimental', description='Gemini 2.0 Flash Experimental', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash', display_name='Gemini 2.0 Flash', description='Gemini 2.0 Flash', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-001', display_name='Gemini 2.0 Flash 001', description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-exp-image-generation', display_name='Gemini 2.0 Flash (Image Generation) Experimental', description='Gemini 2.0 Flash (Image Generation) Experimental', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-lite-001', display_name='Gemini 2.0 Flash-Lite 001', description='Stable version of Gemini 2.0 Flash Lite', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-lite', display_name='Gemini 2.0 Flash-Lite', description='Gemini 2.0 Flash-Lite', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-preview-image-generation', display_name='Gemini 2.0 Flash Preview Image Generation', description='Gemini 2.0 Flash Preview Image Generation', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=32768, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview-02-05', display_name='Gemini 2.0 Flash-Lite Preview 02-05', description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite', version='preview-02-05', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview', display_name='Gemini 2.0 Flash-Lite Preview', description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite', version='preview-02-05', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=8192, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-pro-exp', display_name='Gemini 2.0 Pro Experimental', description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro', version='2.5-exp-03-25', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-pro-exp-02-05', display_name='Gemini 2.0 Pro Experimental 02-05', description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro', version='2.5-exp-03-25', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-exp-1206', display_name='Gemini Experimental 1206', description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro', version='2.5-exp-03-25', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-01-21', display_name='Gemini 2.5 Flash Preview 04-17', description='Preview release (April 17th, 2025) of Gemini 2.5 Flash', version='2.5-preview-04-17', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp', display_name='Gemini 2.5 Flash Preview 04-17', description='Preview release (April 17th, 2025) of Gemini 2.5 Flash', version='2.5-preview-04-17', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-1219', display_name='Gemini 2.5 Flash Preview 04-17', description='Preview release (April 17th, 2025) of Gemini 2.5 Flash', version='2.5-preview-04-17', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=65536, supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-flash-preview-tts', display_name='Gemini 2.5 Flash Preview TTS', description='Gemini 2.5 Flash Preview TTS', version='gemini-2.5-flash-exp-tts-2025-05-19', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=32768, output_token_limit=8192, supported_actions=['countTokens', 'generateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-2.5-pro-preview-tts', display_name='Gemini 2.5 Pro Preview TTS', description='Gemini 2.5 Pro Preview TTS', version='gemini-2.5-pro-preview-tts-2025-05-19', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=65536, output_token_limit=65536, supported_actions=['countTokens', 'generateContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/learnlm-2.0-flash-experimental', display_name='LearnLM 2.0 Flash Experimental', description='LearnLM 2.0 Flash Experimental', version='2.0', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=1048576, output_token_limit=32768, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemma-3-1b-it', display_name='Gemma 3 1B', description=None, version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=32768, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemma-3-4b-it', display_name='Gemma 3 4B', description=None, version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=32768, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemma-3-12b-it', display_name='Gemma 3 12B', description=None, version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=32768, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemma-3-27b-it', display_name='Gemma 3 27B', description=None, version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=131072, output_token_limit=8192, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemma-3n-e4b-it', display_name='Gemma 3n E4B', description=None, version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=8192, output_token_limit=2048, supported_actions=['generateContent', 'countTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/embedding-001', display_name='Embedding 001', description='Obtain a distributed representation of a text.', version='001', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=2048, output_token_limit=1, supported_actions=['embedContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/text-embedding-004', display_name='Text Embedding 004', description='Obtain a distributed representation of a text.', version='004', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=2048, output_token_limit=1, supported_actions=['embedContent'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-embedding-exp-03-07', display_name='Gemini Embedding Experimental 03-07', description='Obtain a distributed representation of a text.', version='exp-03-07', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=8192, output_token_limit=1, supported_actions=['embedContent', 'countTextTokens'], default_checkpoint_id=None, checkpoints=None),\n",
       " Model(name='models/gemini-embedding-exp', display_name='Gemini Embedding Experimental', description='Obtain a distributed representation of a text.', version='exp-03-07', endpoints=None, labels=None, tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None), input_token_limit=8192, output_token_limit=1, supported_actions=['embedContent', 'countTextTokens'], default_checkpoint_id=None, checkpoints=None)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c47ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/medgemma-3-27b-it is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39mGEMINI_API_KEY)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedgemma-3-27b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mPlease follow the instruction step-by-step to generate a better prompt pair.\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;43m    1. Cross over the following prompts and generate a new prompt:\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;43m    Prompt Pair 1: (\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSmall mature lymphocytes centrally show no indication of active replication.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLarge immature cells centrally demonstrate ongoing active replication.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;43m    Prompt Pair 2: (\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSmall, mature lymphocytes centrally show no active processes of chromosome separation.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLarge, immature cells centrally demonstrate active processes of chromosome separation.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;43m    2. Mutate the prompt generated in Step 1 and generate a final prompt pair in a python tuple (str, str)AIzaSyA_c7-szHsOoqDgSRDuZ8drbUbjQWFlAUs\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32mc:\\Users\\kaush\\pyenv\\ml_env\\Lib\\site-packages\\google\\genai\\models.py:5930\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   5928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   5929\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5930\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5931\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_config\u001b[49m\n\u001b[0;32m   5932\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5933\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   5934\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kaush\\pyenv\\ml_env\\Lib\\site-packages\\google\\genai\\models.py:4893\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   4890\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[0;32m   4891\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[1;32m-> 4893\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4894\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[0;32m   4895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[0;32m   4898\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[0;32m   4899\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[0;32m   4900\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\kaush\\pyenv\\ml_env\\Lib\\site-packages\\google\\genai\\_api_client.py:755\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[1;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    747\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    750\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    751\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[BaseResponse, Any]:\n\u001b[0;32m    752\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[0;32m    753\u001b[0m       http_method, path, request_dict, http_options\n\u001b[0;32m    754\u001b[0m   )\n\u001b[1;32m--> 755\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    756\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[0;32m    757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[1;32mc:\\Users\\kaush\\pyenv\\ml_env\\Lib\\site-packages\\google\\genai\\_api_client.py:684\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    677\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    678\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    679\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    682\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    683\u001b[0m   )\n\u001b[1;32m--> 684\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    685\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m    686\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m    687\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\kaush\\pyenv\\ml_env\\Lib\\site-packages\\google\\genai\\errors.py:101\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[1;34m(cls, response)\u001b[0m\n\u001b[0;32m     99\u001b[0m status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m--> 101\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m    103\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[1;31mClientError\u001b[0m: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/medgemma-3-27b-it is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}"
     ]
    }
   ],
   "source": [
    "import util\n",
    "\n",
    "from API_KEY import GEMINI_API_KEY\n",
    "import re\n",
    "import ast\n",
    "from typing import List, Any\n",
    "\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "for i in range(1):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"medgemma-3-27b-it\", contents=\"\"\"Please follow the instruction step-by-step to generate a better prompt pair.\n",
    "    1. Cross over the following prompts and generate a new prompt:\n",
    "\n",
    "    Prompt Pair 1: ('Small mature lymphocytes centrally show no indication of active replication.', 'Large immature cells centrally demonstrate ongoing active replication.')\n",
    "    Prompt Pair 2: ('Small, mature lymphocytes centrally show no active processes of chromosome separation.', 'Large, immature cells centrally demonstrate active processes of chromosome separation.')\n",
    "\n",
    "    2. Mutate the prompt generated in Step 1 and generate a final prompt pair in a python tuple (str, str)\"\"\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5e62b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response on attempt 1: 1. **Cross over the prompts:**\n",
      "\n",
      "   * **Prompt 1 (P1):** \"Small mature lymphocytes centrally show no indication of active replication.\"\n",
      "   * **Prompt 2 (P2):** \"Large immature cells centrally demonstrate ongoing active replication.\"\n",
      "   * **Prompt 3 (P3):** \"Small, mature lymphocytes centrally show no active processes of chromosome separation.\"\n",
      "   * **Prompt 4 (P4):** \"Large, immature cells centrally demonstrate active processes of chromosome separation.\"\n",
      "\n",
      "   Let's combine elements from P1 and P3 for the first part of the new prompt, and P2 and P4 for the second part. We can focus on the core ideas of maturity/immaturity and the presence/absence of replication/division.\n",
      "\n",
      "   **New Prompt (Crossover):** \"Small, mature lymphocytes centrally show no signs of active replication or chromosome separation, while large, immature cells centrally demonstrate active replication and chromosome separation.\"\n",
      "\n",
      "2. **Mutate the prompt generated in Step 1 and generate a final prompt pair:**\n",
      "\n",
      "   Now, let's mutate the crossover prompt to create a more concise and impactful pair. We want to highlight the contrast more starkly and perhaps use slightly different phrasing.\n",
      "\n",
      "   **Final Prompt Pair (Mutation):**\n",
      "\n",
      "   * **Prompt A:** \"Mature, small lymphocytes exhibit no central evidence of proliferation.\"\n",
      "   * **Prompt B:** \"Immature, large cells centrally display vigorous proliferative activity.\"\n",
      "\n",
      "   This pair simplifies the language (\"proliferation\" instead of \"replication or chromosome separation\"), maintains the core contrast (mature/immature, small/large, no activity/vigorous activity), and uses slightly varied vocabulary to make each prompt distinct yet complementary.\n",
      "\n",
      "The final prompt pair is:\n",
      "('Mature, small lymphocytes exhibit no central evidence of proliferation.', 'Immature, large cells centrally display vigorous proliferative activity.')...\n",
      "[Warning] get_prompt_pairs parse error on attempt 1/10: No ```python ... ``` block found\n",
      "Raw response on attempt 2: 1.  **Cross over the prompts:**\n",
      "\n",
      "    The common themes across the prompt pairs are:\n",
      "    * **Cell size and maturity:** \"Small mature lymphocytes\" vs. \"Large immature cells.\"\n",
      "    * **Central location:** \"centrally show/demonstrate.\"\n",
      "    * **Cellular activity/replication:** \"no indication of active replication,\" \"ongoing active replication,\" \"no active processes of chromosome separation,\" \"active processes of chromosome separation.\"\n",
      "\n",
      "    Let's combine these elements to create a new prompt. We can take \"Small mature lymphocytes centrally show no indication of active replication\" and \"Large, immature cells centrally demonstrate active processes of chromosome separation.\"\n",
      "\n",
      "    A new prompt could be: **\"Small mature lymphocytes centrally exhibit no signs of active replication, whereas large immature cells centrally display active processes of cell division.\"**\n",
      "\n",
      "2.  **Mutate the prompt and generate a final prompt pair:**\n",
      "\n",
      "    Let's refine the generated prompt and create a contrasting pair. We can focus on the core difference between the cells' replicative status.\n",
      "\n",
      "    * For the first part, we can simplify \"no signs of active replication\" to \"quiescent.\"\n",
      "    * For the second part, \"active processes of cell division\" can be more specifically described as \"robust mitotic activity.\"\n",
      "\n",
      "    This leads to the final prompt pair:\n",
      "\n",
      "    ('Small, mature lymphocytes centrally appear quiescent.', 'Large, immature cells centrally demonstrate robust mitotic activity.')...\n",
      "[Warning] get_prompt_pairs parse error on attempt 2/10: No ```python ... ``` block found\n",
      "Raw response on attempt 3: 1. **Cross over the prompts:**\n",
      "\n",
      "Let's take elements from all four prompts to create a new one. We can combine the \"small mature lymphocytes\" and \"large immature cells\" dichotomy with aspects of \"active replication\" and \"chromosome separation.\"\n",
      "\n",
      "New prompt idea: \"Small, mature lymphocytes centrally show no signs of active replication or chromosome separation, while large, immature cells centrally demonstrate active replication and chromosome separation.\"\n",
      "\n",
      "2. **Mutate the new prompt and generate a final prompt pair:**\n",
      "\n",
      "Now, let's mutate this into a pair. We want to maintain the core contrast but introduce slight variations in wording or emphasis.\n",
      "\n",
      "* For the first part, we can simplify \"active replication or chromosome separation\" to a more general term for cell division.\n",
      "* For the second part, we can emphasize the *presence* of these active processes.\n",
      "\n",
      "Final Prompt Pair:\n",
      "('Small, mature lymphocytes centrally exhibit no evidence of active cell division.', 'Large, immature cells centrally demonstrate pronounced active cell division.')...\n",
      "[Warning] get_prompt_pairs parse error on attempt 3/10: No ```python ... ``` block found\n",
      "Raw response on attempt 4: 1. **Cross over the prompts:**\n",
      "\n",
      "Let's combine elements from all four prompts to create a new one that incorporates the ideas of replication, chromosome separation, cell size, maturity, and central location. A good crossover prompt would be:\n",
      "\n",
      "\"Small, mature lymphocytes centrally show no active replication or chromosome separation.\"\n",
      "\n",
      "The counterpart to this new prompt should reflect the opposite characteristics, drawing from the \"large immature cells\" and \"active replication/chromosome separation\" aspects. So the second prompt in the pair would be:\n",
      "\n",
      "\"Large, immature cells centrally demonstrate active replication and chromosome separation.\"\n",
      "\n",
      "2. **Mutate the prompt generated in Step 1:**\n",
      "\n",
      "Now, let's mutate the prompt pair:\n",
      "(\"Small, mature lymphocytes centrally show no active replication or chromosome separation.\", \"Large, immature cells centrally demonstrate active replication and chromosome separation.\")\n",
      "\n",
      "We can refine the wording to be more concise and perhaps slightly more scientific in its phrasing, while maintaining the core meaning.\n",
      "\n",
      "* For the first prompt, instead of \"no active replication or chromosome separation,\" we could say \"lacking active proliferation\" or \"exhibiting no signs of proliferation.\"\n",
      "* For the second prompt, \"demonstrate active replication and chromosome separation\" could be \"undergoing active proliferation\" or \"displaying active mitotic processes.\"\n",
      "\n",
      "Let's go with a slight mutation to emphasize the process more directly.\n",
      "\n",
      "Mutated Prompt Pair:\n",
      "('Small, mature lymphocytes centrally lack active proliferation.', 'Large, immature cells centrally demonstrate active mitotic processes.')...\n",
      "[Warning] get_prompt_pairs parse error on attempt 4/10: No ```python ... ``` block found\n",
      "Raw response on attempt 5: 1. **Crossover:**\n",
      "\n",
      "Let's break down the shared and differing elements of the prompts:\n",
      "\n",
      "* **Shared Core:** \"Small... lymphocytes centrally\" and \"Large... cells centrally\" seem to be the consistent structural elements.\n",
      "* **Varying Descriptors:**\n",
      "    * \"mature\" vs. \"immature\"\n",
      "    * \"no indication of active replication\" vs. \"ongoing active replication\"\n",
      "    * \"no active processes of chromosome separation\" vs. \"active processes of chromosome separation\"\n",
      "\n",
      "To cross them over, we can combine the ideas of \"replication\" and \"chromosome separation\" as they are closely related processes indicative of cell division.\n",
      "\n",
      "New Prompt (Crossover): \"Small, mature lymphocytes centrally show no active signs of replication or chromosome separation.\"\n",
      "\n",
      "2. **Mutate:**\n",
      "\n",
      "Now, let's mutate the crossover prompt to create a final pair. We'll maintain the contrasting nature.\n",
      "\n",
      "Final Prompt Pair:\n",
      "('Small, mature lymphocytes centrally exhibit no active processes of cellular proliferation.', 'Large, immature cells centrally demonstrate active processes of cellular proliferation.')...\n",
      "[Warning] get_prompt_pairs parse error on attempt 5/10: No ```python ... ``` block found\n",
      "Raw response on attempt 6: 1. **Cross over the prompts:**\n",
      "\n",
      "Let's combine elements from all four prompts to create a new one. The common themes are cell size, maturity, central activity, and replication/chromosome separation.\n",
      "\n",
      "* From Prompt Pair 1: \"Small mature lymphocytes centrally show no indication of active replication.\"\n",
      "* From Prompt Pair 1: \"Large immature cells centrally demonstrate ongoing active replication.\"\n",
      "* From Prompt Pair 2: \"Small, mature lymphocytes centrally show no active processes of chromosome separation.\"\n",
      "* From Prompt Pair 2: \"Large, immature cells centrally demonstrate active processes of chromosome separation.\"\n",
      "\n",
      "A good crossover would synthesize these ideas. Let's aim for a prompt that reflects the core distinction: small, mature, inactive vs. large, immature, active.\n",
      "\n",
      "New Prompt (Crossover): \"Small, mature cells centrally show no active processes of replication or chromosome separation, while large, immature cells centrally demonstrate active processes of replication and chromosome separation.\"\n",
      "\n",
      "2. **Mutate the prompt and generate a final prompt pair:**\n",
      "\n",
      "Now, let's mutate the crossover prompt to create a refined pair. We want to maintain the core meaning but perhaps make it more concise or introduce slight variations in wording for the positive and negative cases.\n",
      "\n",
      "Original Crossover: \"Small, mature cells centrally show no active processes of replication or chromosome separation, while large, immature cells centrally demonstrate active processes of replication and chromosome separation.\"\n",
      "\n",
      "Let's break it into two distinct statements for the pair and refine the language.\n",
      "\n",
      "* For the \"no active processes\" part, we can simplify \"no active processes of replication or chromosome separation\" to something like \"no active proliferation.\"\n",
      "* For the \"active processes\" part, we can similarly simplify to \"active proliferation.\"\n",
      "\n",
      "Final Prompt Pair:\n",
      "('Small, mature cells centrally show no active proliferation.', 'Large, immature cells centrally demonstrate active proliferation.')...\n",
      "[Warning] get_prompt_pairs parse error on attempt 6/10: No ```python ... ``` block found\n",
      "Raw response on attempt 7: 1. **Crossover:**\n",
      "\n",
      "Let's break down the common elements and differences in the provided prompt pairs.\n",
      "\n",
      "* **Commonality:** Both pairs describe the characteristics of cells, specifically lymphocytes and \"large, immature cells,\" focusing on their central activity related to replication or chromosome separation. The structure \"Small [adjective] cells centrally show no [activity]\" and \"Large [adjective] cells centrally demonstrate [activity]\" is consistent.\n",
      "\n",
      "* **Differences:**\n",
      "    * Prompt Pair 1 uses \"active replication.\"\n",
      "    * Prompt Pair 2 uses \"active processes of chromosome separation.\"\n",
      "    * The adjectives for lymphocytes are \"mature\" and \"mature.\"\n",
      "\n",
      "To cross them over, we can combine the concepts. Let's aim for a prompt that synthesizes the idea of cellular division or proliferation. \"Active replication\" is a broader term that encompasses \"chromosome separation.\"\n",
      "\n",
      "A new prompt from crossing over could be:\n",
      "\n",
      "* \"Small, mature lymphocytes centrally show no indication of active cellular division.\"\n",
      "* \"Large, immature cells centrally demonstrate ongoing active cellular division.\"\n",
      "\n",
      "2.  **Mutate:**\n",
      "\n",
      "Now, let's mutate this new prompt to refine it. We can make the language more precise or introduce a slightly different nuance while maintaining the core contrast.\n",
      "\n",
      "Consider alternatives for \"cellular division\" or ways to describe the \"no indication\" and \"ongoing active\" parts.\n",
      "\n",
      "* Instead of \"ongoing active cellular division,\" we could use something like \"robust mitotic activity\" or \"significant proliferative activity.\"\n",
      "* Instead of \"no indication of active cellular division,\" we could use \"lack proliferative capacity\" or \"exhibit no mitotic figures.\"\n",
      "\n",
      "Let's try to make the second part of the prompt a bit more specific regarding the *type* of activity, while keeping the first part concise.\n",
      "\n",
      "**Final Prompt Pair:**\n",
      "\n",
      "```python\n",
      "(\"Small, mature lymphocytes centrally exhibit no proliferative activity.\", \"Large, immature cells centrally demonstrate robust mitotic figures.\")\n",
      "```...\n",
      "Loaded 2 prompt-pairs.\n",
      "First pair: Small, mature lymphocytes centrally exhibit no proliferative activity.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Small, mature lymphocytes centrally exhibit no proliferative activity.',\n",
       " 'Large, immature cells centrally demonstrate robust mitotic figures.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_and_parse_prompt_list(code: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    From a string of Python code, finds the first occurrence of\n",
    "        = [ ... ]\n",
    "    and parses that bracketed literal into a List[Tuple[str,str]].\n",
    "\n",
    "    Raises:\n",
    "        ValueError if no list literal is found or it’s malformed.\n",
    "    \"\"\"\n",
    "    # 1) grab everything from the first '=' up to the matching ']' \n",
    "    m = re.search(r'=\\s*(\\[\\s*[\\s\\S]*?\\])', code)\n",
    "    if not m:\n",
    "        raise ValueError(\"No list literal found after an '=' in the code\")\n",
    "    list_str = m.group(1)\n",
    "\n",
    "    # 2) safely evaluate it (only literals)\n",
    "    try:\n",
    "        data: Any = ast.literal_eval(list_str)\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        raise ValueError(f\"Malformed list literal: {e}\")\n",
    "\n",
    "    # 3) validate shape\n",
    "    if not isinstance(data, list) or not all(\n",
    "        isinstance(item, (list, tuple)) and len(item) == 2 for item in data\n",
    "    ):\n",
    "        raise ValueError(\"Parsed object is not a list of 2-element lists/tuples\")\n",
    "\n",
    "    # 4) convert to List[Tuple[str,str]]\n",
    "    return [(str(a), str(b)) for a, b in data]\n",
    "\n",
    "def extract_and_parse_prompt_tuple(code: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    From a string of Python code, finds the first literal tuple of two strings\n",
    "    (e.g. (\"neg prompt\",\"pos prompt\")) and returns it as (str, str).\n",
    "\n",
    "    Raises:\n",
    "        ValueError if no suitable 2-element string tuple is found.\n",
    "    \"\"\"\n",
    "    # Parse into an AST\n",
    "    tree = ast.parse(code)\n",
    "\n",
    "    # Walk the tree looking for a Tuple node with exactly two string constants\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Tuple) and len(node.elts) == 2:\n",
    "            a, b = node.elts\n",
    "            if (\n",
    "                isinstance(a, ast.Constant) and isinstance(a.value, str)\n",
    "                and isinstance(b, ast.Constant) and isinstance(b.value, str)\n",
    "            ):\n",
    "                return (a.value, b.value)\n",
    "\n",
    "    raise ValueError(\"No 2-element string tuple found in code\")\n",
    "\n",
    "import io\n",
    "import tokenize\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "\n",
    "def _force_double_quotes(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrites every Python string-literal in `code` to use double-quotes,\n",
    "    properly handling apostrophes and other special characters.\n",
    "    \"\"\"\n",
    "    tokens = tokenize.generate_tokens(io.StringIO(code).readline)\n",
    "    new_tokens = []\n",
    "    for toknum, tokval, start, end, line in tokens:\n",
    "        if toknum == tokenize.STRING:\n",
    "            # Get the actual string value\n",
    "            value = ast.literal_eval(tokval)\n",
    "\n",
    "            # Create a new string literal with double quotes\n",
    "            # Properly escape any double quotes or backslashes in the string\n",
    "            # This automatically handles escaping correctly\n",
    "            tokval = json.dumps(value)\n",
    "\n",
    "        new_tokens.append((toknum, tokval))\n",
    "    return tokenize.untokenize(new_tokens)\n",
    "\n",
    "\n",
    "def get_prompt_pairs(prompt, client, parse_func=extract_and_parse_prompt_list,  max_retries=10) -> List[Tuple[str, str]]:\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = client.generate_content(prompt)\n",
    "            raw = response.text\n",
    "            print(f\"Raw response on attempt {attempt}: {raw}...\")\n",
    "\n",
    "            # 1) extract the python block\n",
    "\n",
    "            m = re.search(r'```python\\s*([\\s\\S]*?)\\s*```', raw)\n",
    "            if not m:\n",
    "                raise ValueError(\"No ```python ... ``` block found\")\n",
    "            code = m.group(1)\n",
    "\n",
    "            # 2) normalize all literals to double-quoted form\n",
    "            code = _force_double_quotes(code)\n",
    "\n",
    "            # print(f\"Normalized code on attempt {attempt}: {code}...\")\n",
    "\n",
    "            # 3) convert the string to a list of tuples\n",
    "            prompts_list = parse_func(code)\n",
    "            prompts: List[Tuple[str, str]] = prompts_list \n",
    "            print(f\"Loaded {len(prompts)} prompt-pairs.\")\n",
    "            print(\"First pair:\", prompts[0])\n",
    "            return prompts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"[Warning] get_prompt_pairs parse error on attempt {attempt}/{max_retries}: {e}\")\n",
    "            if attempt == max_retries:\n",
    "                raise RuntimeError(\n",
    "                    \"Failed to parse prompts after multiple attempts\") from e\n",
    "            # otherwise, retry immediately\n",
    "\n",
    "    # Should never reach here\n",
    "    raise RuntimeError(\"Unreachable\")\n",
    "\n",
    "meta_init_prompt = \"\"\"Please follow the instruction step-by-step to generate a better prompt pair.\n",
    "1. Cross over the following prompts and generate a new prompt:\n",
    "\n",
    "Prompt Pair 1: ('Small mature lymphocytes centrally show no indication of active replication.', 'Large immature cells centrally demonstrate ongoing active replication.')\n",
    "Prompt Pair 2: ('Small, mature lymphocytes centrally show no active processes of chromosome separation.', 'Large, immature cells centrally demonstrate active processes of chromosome separation.')\n",
    "\n",
    "2. Mutate the prompt generated in Step 1 and generate a final prompt pair in a python tuple (str, str)\"\"\"\n",
    "\n",
    "get_prompt_pairs(meta_init_prompt, client, parse_func=extract_and_parse_prompt_tuple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defaultenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
