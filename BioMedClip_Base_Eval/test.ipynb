{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f64ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from open_clip import create_model_and_transforms, get_tokenizer\n",
    "from open_clip.factory import HF_HUB_PREFIX, _MODEL_CONFIGS\n",
    "\n",
    "\n",
    "# Load the model and config files\n",
    "model_name = \"biomedclip_local\"\n",
    "\n",
    "with open(\"checkpoints/open_clip_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    model_cfg = config[\"model_cfg\"]\n",
    "    preprocess_cfg = config[\"preprocess_cfg\"]\n",
    "\n",
    "\n",
    "if (not model_name.startswith(HF_HUB_PREFIX)\n",
    "    and model_name not in _MODEL_CONFIGS\n",
    "    and config is not None):\n",
    "    _MODEL_CONFIGS[model_name] = model_cfg\n",
    "\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "model, _, preprocess = create_model_and_transforms(\n",
    "    model_name=model_name,\n",
    "    pretrained=\"checkpoints/open_clip_pytorch_model.bin\",\n",
    "    **{f\"image_{k}\": v for k, v in preprocess_cfg.items()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8368b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squamous_cell_carcinoma_histopathology.jpeg:\n",
      "squamous cell carcinoma histopathology: 0.9993261098861694\n",
      "adenocarcinoma histopathology: 0.0005834370385855436\n",
      "immunohistochemistry histopathology: 5.3123229008633643e-05\n",
      "hematoxylin and eosin histopathology: 3.730620301212184e-05\n",
      "pie chart: 2.1773455260998276e-10\n",
      "covid line chart: 9.560737618263815e-11\n",
      "brain MRI: 9.659328024935743e-12\n",
      "chest X-ray: 7.296285564660845e-14\n",
      "bone X-ray: 4.939202597809868e-14\n",
      "\n",
      "\n",
      "H_and_E_histopathology.jpg:\n",
      "adenocarcinoma histopathology: 0.9988821148872375\n",
      "squamous cell carcinoma histopathology: 0.0010510372230783105\n",
      "pie chart: 4.1645635064924136e-05\n",
      "chest X-ray: 1.2630177479877602e-05\n",
      "brain MRI: 9.431676517124288e-06\n",
      "hematoxylin and eosin histopathology: 2.600068455649307e-06\n",
      "immunohistochemistry histopathology: 4.84693998714647e-07\n",
      "covid line chart: 3.019886563038199e-09\n",
      "bone X-ray: 6.696965898500551e-11\n",
      "\n",
      "\n",
      "bone_X-ray.jpg:\n",
      "chest X-ray: 0.9959741234779358\n",
      "bone X-ray: 0.00390210235491395\n",
      "pie chart: 6.584016227861866e-05\n",
      "covid line chart: 5.6392280384898186e-05\n",
      "brain MRI: 1.5656665937058278e-06\n",
      "hematoxylin and eosin histopathology: 3.169487783338809e-08\n",
      "adenocarcinoma histopathology: 1.3459727554732126e-08\n",
      "squamous cell carcinoma histopathology: 1.9638979331659812e-10\n",
      "immunohistochemistry histopathology: 1.3248235841700762e-10\n",
      "\n",
      "\n",
      "adenocarcinoma_histopathology.jpg:\n",
      "covid line chart: 0.9059962034225464\n",
      "squamous cell carcinoma histopathology: 0.0364261232316494\n",
      "hematoxylin and eosin histopathology: 0.03523873910307884\n",
      "adenocarcinoma histopathology: 0.019162004813551903\n",
      "immunohistochemistry histopathology: 0.002943165600299835\n",
      "brain MRI: 0.00015511736273765564\n",
      "pie chart: 6.767353625036776e-05\n",
      "bone X-ray: 1.0528292477829382e-05\n",
      "chest X-ray: 4.4289717493484204e-07\n",
      "\n",
      "\n",
      "covid_line_chart.png:\n",
      "chest X-ray: 0.5336702466011047\n",
      "covid line chart: 0.328933984041214\n",
      "brain MRI: 0.12515133619308472\n",
      "immunohistochemistry histopathology: 0.005439193919301033\n",
      "pie chart: 0.004504986107349396\n",
      "adenocarcinoma histopathology: 0.0018980243476107717\n",
      "squamous cell carcinoma histopathology: 0.00031726976158097386\n",
      "hematoxylin and eosin histopathology: 4.2913925426546484e-05\n",
      "bone X-ray: 4.205897494102828e-05\n",
      "\n",
      "\n",
      "IHC_histopathology.jpg:\n",
      "immunohistochemistry histopathology: 0.999106228351593\n",
      "chest X-ray: 0.0006149251130409539\n",
      "brain MRI: 0.00015569939569104463\n",
      "squamous cell carcinoma histopathology: 0.00012222732766531408\n",
      "covid line chart: 4.569701843593066e-07\n",
      "adenocarcinoma histopathology: 2.607144153898844e-07\n",
      "pie chart: 1.4251965296807612e-07\n",
      "bone X-ray: 8.968436548961733e-10\n",
      "hematoxylin and eosin histopathology: 3.0730726297001354e-10\n",
      "\n",
      "\n",
      "chest_X-ray.jpg:\n",
      "bone X-ray: 0.8863083124160767\n",
      "pie chart: 0.07881707698106766\n",
      "chest X-ray: 0.03127392381429672\n",
      "covid line chart: 0.0032930991146713495\n",
      "hematoxylin and eosin histopathology: 0.00029807398095726967\n",
      "brain MRI: 6.749248768755933e-06\n",
      "squamous cell carcinoma histopathology: 1.3652456800627988e-06\n",
      "immunohistochemistry histopathology: 1.1050267403334146e-06\n",
      "adenocarcinoma histopathology: 2.4649847318869433e-07\n",
      "\n",
      "\n",
      "brain_MRI.jpg:\n",
      "brain MRI: 0.9999754428863525\n",
      "chest X-ray: 2.446458711347077e-05\n",
      "covid line chart: 3.219523136976932e-08\n",
      "pie chart: 2.3797802839453652e-08\n",
      "adenocarcinoma histopathology: 1.4649290669410675e-09\n",
      "squamous cell carcinoma histopathology: 7.33928362528502e-10\n",
      "immunohistochemistry histopathology: 4.6128756370222845e-10\n",
      "hematoxylin and eosin histopathology: 2.422853306871531e-10\n",
      "bone X-ray: 3.586060268179203e-11\n",
      "\n",
      "\n",
      "pie_chart.png:\n",
      "pie chart: 0.886633038520813\n",
      "covid line chart: 0.0833565816283226\n",
      "immunohistochemistry histopathology: 0.029302073642611504\n",
      "squamous cell carcinoma histopathology: 0.00040483757038600743\n",
      "brain MRI: 0.0001578655355842784\n",
      "hematoxylin and eosin histopathology: 0.00010888799442909658\n",
      "chest X-ray: 3.067770376219414e-05\n",
      "bone X-ray: 5.001372755941702e-06\n",
      "adenocarcinoma histopathology: 1.0344526799599407e-06\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot image classification\n",
    "template = 'this is a photo of a'\n",
    "labels = [\n",
    "    'adenocarcinoma histopathology',\n",
    "    'brain MRI',\n",
    "    'covid line chart',\n",
    "    'squamous cell carcinoma histopathology',\n",
    "    'immunohistochemistry histopathology',\n",
    "    'bone X-ray',\n",
    "    'chest X-ray',\n",
    "    'pie chart',\n",
    "    'hematoxylin and eosin histopathology'\n",
    "]\n",
    "\n",
    "dataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\n",
    "test_imgs = [\n",
    "    'squamous_cell_carcinoma_histopathology.jpeg',\n",
    "    'H_and_E_histopathology.jpg',\n",
    "    'bone_X-ray.jpg',\n",
    "    'adenocarcinoma_histopathology.jpg',\n",
    "    'covid_line_chart.png',\n",
    "    'IHC_histopathology.jpg',\n",
    "    'chest_X-ray.jpg',\n",
    "    'brain_MRI.jpg',\n",
    "    'pie_chart.png'\n",
    "]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "context_length = 256\n",
    "\n",
    "images = torch.stack([preprocess(Image.open(urlopen(dataset_url + img))) for img in test_imgs]).to(device)\n",
    "texts = tokenizer([template + l for l in labels], context_length=context_length).to(device)\n",
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(images, texts)\n",
    "\n",
    "    logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n",
    "    sorted_indices = torch.argsort(logits, dim=-1, descending=True)\n",
    "\n",
    "    logits = logits.cpu().numpy()\n",
    "    sorted_indices = sorted_indices.cpu().numpy()\n",
    "\n",
    "top_k = -1\n",
    "\n",
    "for i, img in enumerate(test_imgs):\n",
    "    pred = labels[sorted_indices[i][0]]\n",
    "\n",
    "    top_k = len(labels) if top_k == -1 else top_k\n",
    "    print(img.split('/')[-1] + ':')\n",
    "    for j in range(top_k):\n",
    "        jth_index = sorted_indices[i][j]\n",
    "        print(f'{labels[jth_index]}: {logits[i][jth_index]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebaeb43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 2488,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(\"normal\", context_length=context_length)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a685796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 455954 entries, 0 to 455953\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   patient  455954 non-null  int64\n",
      " 1   node     455954 non-null  int64\n",
      " 2   x_coord  455954 non-null  int64\n",
      " 3   y_coord  455954 non-null  int64\n",
      " 4   tumor    455954 non-null  int64\n",
      " 5   slide    455954 non-null  int64\n",
      " 6   center   455954 non-null  int64\n",
      " 7   split    455954 non-null  int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 31.3 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata_csv = \"/home/E19_FYP_Domain_Gen_Data/metadata.csv\" \n",
    "patches_dir = \"/home/E19_FYP_Domain_Gen_Data/patches\"       \n",
    "metadata_df = pd.read_csv(metadata_csv, index_col=[0])\n",
    "metadata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0ee283c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "0    410359\n",
       "1     45595\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e77a9",
   "metadata": {},
   "source": [
    "# Selecting a source domain (Center 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c797e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "0    53425\n",
       "1     6011\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center0_df = metadata_df[metadata_df['center'] == 0].reset_index(drop=True)\n",
    "center0_df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b898af",
   "metadata": {},
   "source": [
    "# Testing on Center 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/e19372/anaconda3/envs/biomedclip/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Evaluating: 100%|██████████| 12/12 [00:18<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:    0.8197\n",
      "ROC AUC:          0.0937\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2285  747]\n",
      " [ 337 2642]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8715    0.7536    0.8083      3032\n",
      "           1     0.7796    0.8869    0.8298      2979\n",
      "\n",
      "    accuracy                         0.8197      6011\n",
      "   macro avg     0.8255    0.8203    0.8190      6011\n",
      "weighted avg     0.8259    0.8197    0.8189      6011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from open_clip import create_model_and_transforms, get_tokenizer\n",
    "from open_clip.factory import HF_HUB_PREFIX, _MODEL_CONFIGS\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Paths & constants\n",
    "METADATA_CSV = \"/home/E19_FYP_Domain_Gen_Data/metadata.csv\"\n",
    "PATCHES_DIR  = \"/home/E19_FYP_Domain_Gen_Data/patches\"\n",
    "CONFIG_PATH  = \"checkpoints/open_clip_config.json\"\n",
    "WEIGHTS_PATH = \"checkpoints/open_clip_pytorch_model.bin\"\n",
    "MODEL_NAME   = \"biomedclip_local\"\n",
    "CONTEXT_LENGTH = 256\n",
    "BATCH_SIZE   = 512\n",
    "NUM_WORKERS  = 4\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Load metadata and filter center=0\n",
    "metadata_df = pd.read_csv(METADATA_CSV, index_col=0)\n",
    "center0_df  = metadata_df[metadata_df.center == 0].copy()\n",
    "\n",
    "# 3. Build filenames and full filepaths\n",
    "center0_df[\"filename\"] = center0_df.apply(\n",
    "    lambda r: f\"patch_patient_{r.patient:03d}_node_{r.node}_x_{r.x_coord}_y_{r.y_coord}.png\",\n",
    "    axis=1\n",
    ")\n",
    "center0_df[\"filepath\"] = center0_df.apply(\n",
    "    lambda r: os.path.join(\n",
    "        PATCHES_DIR,\n",
    "        f\"patient_{r.patient:03d}_node_{r.node}\",\n",
    "        r.filename\n",
    "    ), \n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "# 4. Split into train/val/test\n",
    "train_df = center0_df[center0_df.split == 0]\n",
    "test_df  = center0_df[center0_df.split  == 1]\n",
    "\n",
    "# 5. Define a Dataset for loading & preprocessing\n",
    "class BiomedCLIPDataset(Dataset):\n",
    "    def __init__(self, df, preprocess):\n",
    "        self.filepaths = df[\"filepath\"].tolist()\n",
    "        self.labels    = df[\"tumor\"].astype(int).tolist()\n",
    "        self.preproc   = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.filepaths[idx]).convert(\"RGB\")\n",
    "        img = self.preproc(img)           # yields a torch.Tensor (C,H,W)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "# 6. Load BiomedCLIP model + tokenizer + preprocess\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "model_cfg, preproc_cfg = cfg[\"model_cfg\"], cfg[\"preprocess_cfg\"]\n",
    "\n",
    "# register local config if needed\n",
    "if (not MODEL_NAME.startswith(HF_HUB_PREFIX)\n",
    "    and MODEL_NAME not in _MODEL_CONFIGS):\n",
    "    _MODEL_CONFIGS[MODEL_NAME] = model_cfg\n",
    "\n",
    "tokenizer = get_tokenizer(MODEL_NAME)\n",
    "model, _, preprocess = create_model_and_transforms(\n",
    "    model_name=MODEL_NAME,\n",
    "    pretrained=WEIGHTS_PATH,\n",
    "    **{f\"image_{k}\": v for k,v in preproc_cfg.items()}\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "# 7. Prepare DataLoader for test set\n",
    "test_ds = BiomedCLIPDataset(test_df, preprocess)\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# 8. Pre-tokenize your two prompts once\n",
    "prompts = [\n",
    "   \"Tumor is not present in this image\"\n",
    "   \"This is an image of a tumor\"\n",
    "]\n",
    "text_inputs = tokenizer(\n",
    "    prompts, context_length=CONTEXT_LENGTH\n",
    ").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_feats = model.encode_text(text_inputs)               # (2, D)\n",
    "    text_feats = text_feats / text_feats.norm(dim=1, keepdim=True)\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "\n",
    "    all_preds  = []\n",
    "    all_probs  = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 9. Inference loop\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        imgs   = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        img_feats = model.encode_image(imgs)                    # (B, D)\n",
    "        img_feats = img_feats / img_feats.norm(dim=1, keepdim=True)\n",
    "\n",
    "        logits = logit_scale * (img_feats @ text_feats.t())     # (B, 2)\n",
    "        probs  = logits.softmax(dim=1)                          # (B, 2)\n",
    "        preds  = logits.argmax(dim=1)                           # (B,)\n",
    "\n",
    "        all_preds .append(preds.cpu())\n",
    "        all_probs .append(probs[:, 0].cpu())  # tumor-class prob\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    # concatenate\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_prob = torch.cat(all_probs).numpy()\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "\n",
    "# 10. Compute & print metrics\n",
    "acc   = accuracy_score(y_true, y_pred)\n",
    "cm    = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, digits=4)\n",
    "auc   = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "print(f\"\\nTest Accuracy:    {acc:.4f}\")\n",
    "print(f\"ROC AUC:          {auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
